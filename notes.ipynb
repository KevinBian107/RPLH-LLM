{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb546a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce02fa-5ab3-43ca-9981-5900aa59f5ac",
   "metadata": {},
   "source": [
    "ollama pull model_name:parameter\n",
    "\n",
    "ollama serve\n",
    "\n",
    "api endpoint: localhost:11434/api/generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e67ad262-b5ca-4830-9833-96837bc28dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume the model is deployed locally\n",
    "def request_function(url, prompt):\n",
    "    '''\n",
    "    url: localhost url or url from other device\n",
    "    '''\n",
    "    data = {\n",
    "        \"model\": \"llama3.2\", \n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 50,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response=requests.post(url=url, json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print('Error:', response.status_code, response.json())\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "324d7b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtracking Line Search (BLS) is an optimization algorithm used to find the minimum or maximum of a convex function. It is particularly useful when combined with gradient-based methods, such as quasi-Newton methods.\n",
      "\n",
      "The BLS algorithm works by iteratively moving along the direction of negative curvature (i.e., the direction in which the curvature changes from positive to negative) until it reaches a point where the curvature changes sign for the first time. At this point, it reverses its step and moves in the opposite direction, effectively \"backtracking\" along the line that leads to the maximum or minimum.\n",
      "\n",
      "The BLS algorithm can be formulated as follows:\n",
      "\n",
      "1. Initialize a starting point `x` and a step size `α`.\n",
      "2. Compute the gradient of the function `f` at `x`, denoted by `∇f(x)`.\n",
      "3. Compute the Hessian matrix of `f` at `x`, denoted by `H(x)` or `H_f(x)`.\n",
      "4. Check if the curvature changes sign. If not, proceed to step 5.\n",
      "5. Update the direction `d` using the line search formula:\n",
      "```r\n",
      "d = -α * ∇f(x)\n",
      "```\n",
      "6. Compute the value of the function at the updated point `x + α*d`, denoted by `f(x + α*d)`.\n",
      "7. Check if the curvature changes sign at this new point. If not, proceed to step 5.\n",
      "8. Otherwise, update the step size `α` and repeat steps 2-7 until convergence.\n",
      "\n",
      "The BLS algorithm is particularly useful when:\n",
      "\n",
      "* The function has a large number of local optima or saddle points.\n",
      "* The function has a non-trivial convexity structure (i.e., it has multiple minima or maxima).\n",
      "* The Hessian matrix is not invertible at the current point.\n",
      "\n",
      "However, BLS can be computationally expensive if the step size `α` is too large. In practice, this is often mitigated by using techniques such as line search with an armijo rule, which controls the maximum allowed decrease in function value.\n",
      "\n",
      "BLS has been used successfully in various applications, including:\n",
      "\n",
      "* Optimization of large-scale machine learning models.\n",
      "* Nonlinear least squares problems.\n",
      "* Nonlinear programming problems.\n",
      "\n",
      "In summary, BLS is a powerful optimization algorithm that can handle complex convex functions and is particularly useful when combined with gradient-based methods.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'What is backtracking line search?'\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "response = request_function(url, prompt)\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3567f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = ollama.embeddings(model=\"llama3.2:1b\", prompt=\"Hello Ollama!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae749c43-5616-4e3b-89ae-b9c7b18d8c57",
   "metadata": {},
   "source": [
    "Prompt engineering:\n",
    "\n",
    "Ideally, prompt engineering should be made such that the LLM model will output an action that is available in the action space and we can parse the output to a function call in python. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381b961-3fa8-4d4a-90e8-f2fabc3fc6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
